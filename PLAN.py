import streamlit as st
import fitz  # PyMuPDF
import google.generativeai as genai
import re
import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from docx import Document
import tempfile
import os
import ollama
from google.cloud import vision
import io
from PIL import Image
from difflib import SequenceMatcher 
from dotenv import load_dotenv
# âœ… åˆå§‹åŒ– Gemini API
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY","")

genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel("gemini-2.5-flash")

# âœ… è¼‰å…¥ FAISS å‘é‡åº«
INDEX_FILE_PATH = "faiss_index"


from langchain_community.vectorstores import FAISS

# ä¿®æ”¹å¾Œ
import torch
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2",
    model_kwargs={"device": "cpu"},  # âœ… å¼·åˆ¶ CPUï¼Œé¿é–‹ meta tensor éŒ¯èª¤
    cache_folder="./.cache"          # âœ… é¿å…é‡è¤‡ä¸‹è¼‰
)

vector_store = FAISS.load_local(
    "faiss_index",
    embeddings=embeddings,
    allow_dangerous_deserialization=True
)




import os
import json
from google.cloud import vision
from google.oauth2 import service_account
from google.cloud import vision

import os
import json
import tempfile
import streamlit as st
from google.cloud import vision

import os









# âœ… åœ¨ä¸»ç¨‹å¼ä¸­å‘¼å«
# âœ… åˆ†é¡æç¤ºè©
FACILITY_PROMPT = """
è«‹åˆ¤æ–·ä¸‹åˆ—ç”³è«‹æ–‡ä»¶æ‰€å±¬é†«ç™‚æ©Ÿæ§‹å±¤ç´šï¼Œåƒ…å¾ä¸‹åˆ—äº”é¡ä¸­æ“‡ä¸€ï¼Œ**ä¸å¾—æ£æ„æ–°å¢æ–°åˆ†é¡**ï¼š
ã€Œé†«å­¸ä¸­å¿ƒã€ã€ã€Œé—œéµåŸºç¤è¨­æ–½å€åŸŸé†«é™¢ã€ã€ã€Œéé—œéµåŸºç¤è¨­æ–½å€åŸŸé†«é™¢ã€ã€ã€Œåœ°å€é†«é™¢ã€ã€ã€Œè¨ºæ‰€ã€ã€‚
è«‹åš´æ ¼åªå›è¦†å…¶ä¸­ä¸€å€‹åˆ†é¡åç¨±ï¼Œä¸è¦åŠ ä»»ä½•èªªæ˜æˆ–æ¨™é»ç¬¦è™Ÿã€‚

è‹¥æ–‡ä»¶ä¸­æåŠçš„å€åŸŸé†«é™¢åç¨±å±¬æ–¼ä»¥ä¸‹æ¸…å–®ï¼Œè«‹ç›´æ¥åˆ†é¡ç‚ºã€Œé—œéµåŸºç¤è¨­æ–½å€åŸŸé†«é™¢ã€ï¼š
æŒ¯èˆˆé†«ç™‚è²¡åœ˜æ³•äººæŒ¯èˆˆé†«é™¢
è‡ºåŒ—é†«å­¸å¤§å­¸é™„è¨­é†«é™¢
é†«ç™‚è²¡åœ˜æ³•äººç¾…è¨±åŸºé‡‘æœƒç¾…æ±åšæ„›é†«é™¢
é•·åºšé†«ç™‚è²¡åœ˜æ³•äººåŸºéš†é•·åºšç´€å¿µé†«é™¢
ä½›æ•™æ…ˆæ¿Ÿé†«ç™‚è²¡åœ˜æ³•äººå°åŒ—æ…ˆæ¿Ÿé†«é™¢
è¡›ç”Ÿç¦åˆ©éƒ¨é›™å’Œé†«é™¢
å¤©ä¸»æ•™è€•è˜é†«ç™‚è²¡åœ˜æ³•äººè€•è˜é†«é™¢
æ±å…ƒç¶œåˆé†«é™¢
å¤§åƒç¶œåˆé†«é™¢
æ¾„æ¸…ç¶œåˆé†«é™¢ä¸­æ¸¯åˆ†é™¢
ç«¥ç¶œåˆé†«ç™‚ç¤¾åœ˜æ³•äººç«¥ç¶œåˆé†«é™¢
å…‰ç”°é†«ç™‚ç¤¾åœ˜æ³•äººå…‰ç”°ç¶œåˆé†«é™¢
ç§€å‚³é†«ç™‚ç¤¾åœ˜æ³•äººç§€å‚³ç´€å¿µé†«é™¢
ç§€å‚³é†«ç™‚è²¡åœ˜æ³•äººå½°æ¿±ç§€å‚³ç´€å¿µé†«é™¢
é•·åºšé†«ç™‚è²¡åœ˜æ³•äººå˜‰ç¾©é•·åºšç´€å¿µé†«é™¢
ä½›æ•™æ…ˆæ¿Ÿé†«ç™‚è²¡åœ˜æ³•äººå¤§æ—æ…ˆæ¿Ÿé†«é™¢
æˆ´å¾·æ£®é†«ç™‚è²¡åœ˜æ³•äººå˜‰ç¾©åŸºç£æ•™é†«é™¢
ç¾©å¤§é†«ç™‚è²¡åœ˜æ³•äººç¾©å¤§é†«é™¢
å®‰æ³°é†«ç™‚ç¤¾åœ˜æ³•äººå®‰æ³°é†«é™¢
è‡ºåŒ—å¸‚ç«‹è¯åˆé†«é™¢
è¡›ç”Ÿç¦åˆ©éƒ¨æ¡ƒåœ’é†«é™¢
åœ‹ç«‹å°ç£å¤§å­¸é™„è¨­é†«é™¢åˆ†é™¢æ–°ç«¹é†«é™¢
åœ‹ç«‹å°ç£å¤§å­¸é™„è¨­é†«é™¢é›²æ—åˆ†é™¢
é€£æ±Ÿç¸£ç«‹é†«é™¢
è¡›ç”Ÿç¦åˆ©éƒ¨é‡‘é–€é†«é™¢
è¡›ç”Ÿç¦åˆ©éƒ¨å—æŠ•é†«é™¢
è¡›ç”Ÿç¦åˆ©éƒ¨æ¾æ¹–é†«é™¢
è¡›ç”Ÿç¦åˆ©éƒ¨è‡ºä¸­é†«é™¢
è¡›ç”Ÿç¦åˆ©éƒ¨è‡ºå—é†«é™¢
è¡›ç”Ÿç¦åˆ©éƒ¨å±æ±é†«é™¢
è¡›ç”Ÿç¦åˆ©éƒ¨èŠ±è“®é†«é™¢

è‹¥æ–‡ä»¶ä¸­æåŠçš„å€åŸŸé†«é™¢åç¨±ä¸å±¬æ–¼æ¸…å–®å…§ï¼Œè«‹ç›´æ¥åˆ†é¡ç‚ºã€Œéé—œéµåŸºç¤è¨­æ–½å€åŸŸé†«é™¢ã€ï¼š

---æ–‡ä»¶å…§å®¹---
{text}
---çµæŸ---
"""

REPEATED_SUBSIDY_PROMPT = """
è«‹åœ¨ç›®éŒ„é æˆ–ç”³è«‹å–®ä½è‡ªæˆ‘æª¢æ ¸é …ç›®è¡¨ä¸­æª¢æŸ¥æ˜¯å¦æœ‰é™„ä¸Šã€Œæœªæœ‰é‡è¤‡ç”³è«‹è¨ˆç•«ä¹‹è²æ˜åˆ‡çµæ›¸ã€ï¼Œè‹¥æ²’æœ‰é™„ä¸Šæ‡‰è¦–ç‚ºæœ‰ã€Œé‡è¤‡ç”³è«‹è£œåŠ©ã€ä¹‹æƒ…äº‹

æ˜¯å¦é‡è¤‡ç”³è«‹ï¼š1/0
ç†ç”±ï¼šï¼ˆç°¡è¦èªªæ˜åˆ¤æ–·ä¾æ“šï¼‰

---æ–‡ä»¶å…§å®¹---
{text}
---çµæŸ---
"""

PLAN_PROMPT = """
è«‹æ ¹æ“šä»¥ä¸‹å¥åº·å°ç£æ·±è€•è¨ˆç•«ç”³è«‹æ›¸å…§å®¹ï¼Œ**æº–ç¢ºä¸”ç„¡éŒ¯å­—åœ°**æ“·å–ä¸¦åˆ—å‡ºä¸‹åˆ—è³‡è¨Šï¼Œæ¯ä¸€é …è«‹ä»¥**å›ºå®šæ ¼å¼**å›ç­”ï¼š

ç”³è«‹æ©Ÿæ§‹ï¼šXXX  
è¨ˆç•«åç¨±ï¼šXXXï¼ˆæ³¨æ„ï¼šè«‹å‹¿èª¤å°‡è¨ˆç•«ç¯„ç–‡èª¤èªç‚ºè¨ˆç•«åç¨±ï¼‰  
è¨ˆç•«ç”³è«‹ç·¨è™Ÿï¼šXXXï¼ˆæ‡‰ç‚ºå…©è‡³ä¸‰ä½æ•¸å­—ï¼‰

è«‹åš´æ ¼ä¾ç…§ä¸Šè¿°æ ¼å¼ä½œç­”ï¼Œ**ä¸å¾—åŒ…å«ä»»ä½•å¤šé¤˜å­—å…ƒæˆ–èªªæ˜**ï¼Œä¹Ÿä¸å¾—æœ‰ä»»ä½•éŒ¯å­—æˆ–æ ¼å¼åå·®ã€‚

---æ–‡ä»¶å…§å®¹---
{text}
---çµæŸ---
"""

FACILITY_SCORE_CAP_INFO = {
    "é†«å­¸ä¸­å¿ƒ": 30,
    "é—œéµåŸºç¤è¨­æ–½å€åŸŸé†«é™¢": 30,
    "éé—œéµåŸºç¤è¨­æ–½å€åŸŸé†«é™¢": 26,
    "åœ°å€é†«é™¢": 23,
    "è¨ºæ‰€": 23
}

INFO_QUESTIONS = [
    "1.æ˜¯å¦è¨­ç½®æ©Ÿæ§‹è³‡é€šå®‰å…¨é•·(CISO)ã€è³‡å®‰å°ˆè²¬äººå“¡ï¼Ÿ",
    "2.æ˜¯å¦è¨‚å®šã€ä¿®æ­£åŠå¯¦æ–½è³‡é€šå®‰å…¨ç¶­è­·è¨ˆç•«ï¼Ÿ",
    "3.æ˜¯å¦è¾¦ç†è³‡é€šè¨Šç³»çµ±ç›¤é»åŠé¢¨éšªè©•ä¼°ï¼Ÿ",
    "4.æ˜¯å¦å·²åŠ å…¥ã€Œè¡›ç”Ÿç¦åˆ©éƒ¨è³‡å®‰èˆ‡è³‡è¨Šåˆ†äº«èˆ‡åˆ†æä¸­å¿ƒï¼Ÿ",
    "5.æ˜¯å¦è½å¯¦æœå‹™ä¾›æ‡‰å•†ç®¡ç†ï¼ŒåŒ…å«å°‡è³‡å®‰ç´å…¥å¥‘ç´„åŠå§”å¤–å‰é€²è¡Œè³‡å®‰é¢¨éšªè©•ä¼°ï¼Ÿ",
    "6.æ˜¯å¦è¦æ±‚æ‰€æœ‰äººå“¡æ¯å¹´åƒåŠ è‡³å°‘3å°æ™‚è³‡é€šå®‰å…¨é€šè­˜æ•™è‚²è¨“ç·´ï¼Ÿ",
    "7.æ˜¯å¦å°‡ç¶²è·¯å€éš”ï¼Ÿ",
    "8.æ˜¯å¦å°‡é‡è¦è³‡æ–™å‚™ä»½åŠåŠ å¯†ï¼Ÿ",
    "9.æ˜¯å¦æ–¼ä½¿ç”¨è€…é›»è…¦åŠä¼ºæœå™¨å»ºç½®é˜²æ¯’è»Ÿé«”ï¼Ÿ",
    "10.æ˜¯å¦å»ºç½®ç¶²è·¯é˜²ç«ç‰†ï¼Ÿ",
    "11.æ˜¯å¦è¾¦ç†å¼±é»æƒæï¼Ÿ",
    "12.æ˜¯å¦è¾¦ç†æ»²é€æ¸¬è©¦ï¼Ÿ",
    "13.æ˜¯å¦å·²å»ºç½®æ˜¯å¦å»ºç½®é›»å­éƒµä»¶éæ¿¾æ©Ÿåˆ¶(SPAM)ï¼Ÿ",
    "14.æ˜¯å¦å·²å»ºç½®å…¥ä¾µåµæ¸¬åŠé˜²ç¦¦æ©Ÿåˆ¶ï¼Ÿ",
    "15.æ˜¯å¦å·²å»ºç½®æ‡‰ç”¨ç¨‹å¼é˜²ç«ç‰†ï¼Ÿ",
    "16.æ˜¯å¦å·²å»ºç½®é€²éšæŒçºŒæ€§å¨è„…æ”»æ“Šé˜²ç¦¦æªæ–½ï¼Ÿ",
    "17.æ˜¯å¦å·²å»ºç½®è³‡é€šå®‰å…¨å¨è„…åµæ¸¬ç®¡ç†æ©Ÿåˆ¶(SOC)ï¼Ÿ",
    "18.æ˜¯å¦å·²å°å…¥æ”¿åºœçµ„æ…‹åŸºæº–(GCB)ï¼Ÿ",
    "19.æ˜¯å¦å·²å°å…¥ç«¯é»åµæ¸¬åŠæ‡‰è®Šæ©Ÿåˆ¶(EDR)ï¼Ÿ",
    "20.æ˜¯å¦æ›¾æ–¼æœ¬éƒ¨æˆ–è³‡é€šå®‰å…¨ç½²è³‡é€šå®‰å…¨ç¨½æ ¸è¡¨ç¾ç¸¾å„ªï¼Ÿ",
    "21.æ˜¯å¦æ›¾åƒåŠ æœ¬éƒ¨ç´…è—éšŠæ”»é˜²æ¼”ç·´(åŒ…å«åƒåŠ è©•é¸)ï¼Ÿ",
    "22.æ˜¯å¦åˆ—ç®¡ä¸¦é€å¹´æ±°æ›å¤§é™¸å» æ’è³‡é€šè¨Šç”¢å“ï¼Ÿ",
    "23.æ˜¯å¦å»ºç«‹ FHIR çµæ§‹è³‡æ–™åº«ï¼ˆç¬¦åˆ TW Core IGï¼‰ï¼Ÿ",
    "24.æ˜¯å¦æœ‰è·¨é™¢è³‡æ–™äº¤æ›è¦åŠƒï¼Ÿ",
    "25.æ˜¯å¦è¨­æœ‰è³‡æ–™æ²»ç†æµç¨‹/SOP æˆ–æ¸…ç†è½‰æ›æ©Ÿåˆ¶ï¼Ÿ",
    "26.æ˜¯å¦èƒ½èˆ‡ SMART on FHIRã€CDS hookã€CQL æ¥è»Œï¼Ÿ",
    "27.æ˜¯å¦æ¡ç”¨åœ‹éš›æ¨™æº–å¦‚ LOINCã€SNOMED CTã€RxNormï¼Ÿ",
    "28.æ˜¯å¦åœ¨é†«ç™‚æ•¸æ“šè³‡æ–™æ‡‰ç”¨ä¸­ï¼ŒåŒ…å«TW CDIè³‡æ–™é›†ï¼Ÿ",
    "29.æ˜¯å¦æåŠä¸ƒå¤§å€«ç†åŸå‰‡ï¼ˆè‡ªä¸»ã€é€æ˜ã€ç•¶è²¬ã€å®‰å…¨ã€å…¬å¹³ã€æ°¸çºŒã€éš±ç§ï¼‰ï¼Ÿ",
    "30.æ˜¯å¦è¨­æœ‰ AI ç®¡ç†å§”å“¡æœƒèˆ‡åˆ¶åº¦è¦ç« ï¼Ÿ",
    "31.æ˜¯å¦å°å…¥ AI é€æ˜æ€§èˆ‡æ­éœ²æ©Ÿåˆ¶ï¼Ÿ",
    "32.æ˜¯å¦æœ‰ AI è½åœ°æ²»ç†ç®¡ç†è¾¦æ³•ï¼Ÿ",
    "33.æ˜¯å¦é€²è¡Œæ¨¡å‹æ•ˆèƒ½ç›£æ¸¬èˆ‡å†è¨“ç·´ï¼Ÿ",
    "34.æ˜¯å¦æœ‰å¯¦éš› AI æ‡‰ç”¨æ¡ˆä¾‹ï¼Ÿ",
    "35.æ˜¯å¦ä¾æ“šè² è²¬ä»»AI ä¸­å¿ƒè¦ç¯„é€²è¡Œå°æ¨™å»ºç½®ï¼Ÿ",
    "36.æ˜¯å¦ä¾æ“šè² è²¬ä»»AI ä¸­å¿ƒè¦ç¯„é€²è¡Œå°æ¨™å»ºç½®ï¼Ÿ"
]

def extract_text_by_line(pdf_bytes):
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    lines = []
    for page in doc:
        blocks = page.get_text("blocks")
        for b in blocks:
            text = b[4].strip()
            if text:
                lines.append(text)
    return "\n\n".join(lines)

import fitz  # PyMuPDF
import fitz
from PIL import Image





        

def build_individual_prompts(questions, full_text, role_name="å¯©æŸ¥å§”å“¡"):
    prompts = []

    for q in questions:
        docs = vector_store.similarity_search(q, k=3)
        rag_context = "\n---\n".join(doc.page_content for doc in docs)
        prompt = f"""
ä½ æ˜¯ä¸€ä½{role_name}ï¼Œè«‹é–±è®€ä¸‹æ–¹çš„æ™ºæ…§é†«ç™‚ä¸­å¿ƒæŠ€è¡“æ‰‹å†ŠåŠè¨ˆç•«ç”³è«‹æ–‡ä»¶å…§å®¹ï¼Œä¸¦é‡å°æŒ‡å®šé¡Œç›®ä½œç­”ã€‚  
è«‹é‡å°è©²é¡Œå›ç­”ã€Œå¾—åˆ†ï¼ˆåªèƒ½æ˜¯ 0 æˆ– 1 åˆ†ï¼‰ã€ä»¥åŠã€ŒåŸå› ã€ã€‚

è«‹åš´æ ¼ä¾ç…§ä»¥ä¸‹æ ¼å¼å§”å©‰ä½œç­”ï¼Œä¸è¦æ·»åŠ ä»»ä½•å¤šé¤˜èªªæ˜æˆ–æ ¼å¼è®ŠåŒ–ï¼Œæ ¼å¼éŒ¯èª¤æœƒå°è‡´ç³»çµ±ç„¡æ³•è§£æã€‚

å›ç­”æ ¼å¼å¦‚ä¸‹ï¼š
å¾—åˆ† âŸªxâŸ«/1 ï¼ŒåŸå› ï¼š...(è«‹ç·Šæ¥åœ¨åŒä¸€è¡Œ)

é¡Œç›®ï¼š{q}

---æ™ºæ…§é†«ç™‚ä¸­å¿ƒæŠ€è¡“æ‰‹å†Š---
{rag_context}
---æ–‡ä»¶å…§å®¹é–‹å§‹---
{full_text}
---æ–‡ä»¶å…§å®¹çµæŸ---
"""
        prompts.append(prompt.strip())
    return prompts

def get_gemini_response(prompt):
    try:
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        return f"âš ï¸ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"


from langchain.chains.question_answering import load_qa_chain
from langchain.docstore.document import Document

def query_rag_for_legal_compliance(full_text, vector_store=vector_store, model=model):
    query = "æœ¬æ–‡ä»¶æ˜¯å¦ç¬¦åˆæ”¿åºœè£œåŠ©è¨ˆç•«çš„é©æ³•æ€§ï¼Ÿè«‹æ ¹æ“šæ³•è¦èˆ‡æ–‡ä»¶å…§å®¹çµ¦å‡ºæ˜ç¢ºçµè«–èˆ‡ç°¡è¦ç†ç”±ã€‚"
    docs = vector_store.similarity_search(query, k=3)
    rag_context = "\n---\n".join(doc.page_content for doc in docs)
    rag_prompt = f"""
ä½ æ˜¯ä¸€ä½ç†Ÿæ‚‰æ”¿åºœè£œåŠ©æ³•è¦èˆ‡å¯¦å‹™å¯©æŸ¥çš„è³‡æ·±å¯©æŸ¥å“¡ï¼Œè«‹ä¾æ“šä¸‹åˆ—æ³•è¦èˆ‡æŠ€è¡“è¦ç¯„ï¼Œåš´æ ¼å¯©æŸ¥ä¸‹æ–¹ç”³è«‹æ›¸å…§å®¹æ˜¯å¦ç¬¦åˆæ”¿åºœè£œåŠ©ä¹‹ã€Œè³‡å®‰è¦æ±‚ã€èˆ‡ã€Œæ³•è¦åˆè¦æ€§ã€ã€‚

âš ï¸ è«‹åš´æ ¼ä¾æ“šä»¥ä¸‹æ–‡ä»¶é€²è¡Œå¯©æŸ¥ï¼Œä¸å¾—æ¨è«–ã€è‡†æ¸¬ã€åˆç†åŒ–æˆ–è£œè¶³æœªæ˜è¼‰æ–¼è¨ˆç•«ä¸­çš„å…§å®¹ï¼š
- è¡›ç¦éƒ¨ã€Šå¥åº·å°ç£æ·±è€•è¨ˆç•«ã€‹ç¯„ç–‡ä¸‰ï¼šè³‡å®‰æ²»ç†ã€è³‡æ–™æ²»ç†ã€AIæ²»ç† æ‡¶äººåŒ…èˆ‡å•ç­”é›†
- ã€Šè³‡é€šå®‰å…¨ç®¡ç†æ³•ã€‹èˆ‡ã€Šè³‡é€šå®‰å…¨è²¬ä»»ç­‰ç´šåˆ†ç´šè¾¦æ³•ã€‹
- ã€Šå€‹äººè³‡æ–™ä¿è­·æ³•ã€‹ã€ã€Šé†«ç™‚æ©Ÿæ§‹é›»å­ç—…æ­·è£½ä½œèˆ‡ç®¡ç†è¾¦æ³•ã€‹
- ã€Šè¡›ç¦éƒ¨é†«ç™‚è³‡é€šç³»çµ±è³‡å®‰é˜²è­·åŸºæº–ã€‹

è«‹ä¾æ“šä¸‹åˆ—ä¸‰å¤§æ§‹é¢èˆ‡é†«ç™‚æ©Ÿæ§‹å±¤ç´šï¼Œé€é …åˆ¤æ–·å…¶æ˜¯å¦ç¬¦åˆé©æ³•æ€§æ¨™æº–ï¼š

ä¸€ã€åŸºæœ¬é©æ³•æ€§æ¨™æº–ï¼ˆæ‰€æœ‰é†«ç™‚æ©Ÿæ§‹çš†é ˆç¬¦åˆï¼‰
äºŒã€ç­‰ç´šè²¬ä»»æ¨™æº–ï¼ˆè³‡å®‰æ³•ç´ç®¡ vs éç´ç®¡é†«ç™‚æ©Ÿæ§‹ï¼‰
ä¸‰ã€ç‰¹å®šç³»çµ±èˆ‡è³‡æ–™ä½¿ç”¨é©æ³•æ€§ï¼ˆå¦‚ AI å°å…¥ã€è³‡æ–™äº¤æ›ã€è·¨é™¢å…±äº«ï¼‰

---å‘é‡æŸ¥è©¢çµæœï¼ˆæ³•è¦æ–‡ä»¶æ®µè½ï¼‰---
{rag_context}

---å®Œæ•´ç”³è«‹æ›¸ç¯€éŒ„---
{full_text}

è«‹ä¾ä»¥ä¸‹æ ¼å¼å§”å©‰ä½œç­”ï¼Œå‹™å¿…ä¿æŒæ ¼å¼æ­£ç¢ºï¼š

æ˜¯å¦ç¬¦åˆé©æ³•æ€§ï¼šç¬¦åˆï¼ä¸ç¬¦åˆ  
ç†ç”±ï¼šï¼ˆè«‹å…·é«”åˆ—å‡ºä¸ç¬¦åˆä¹‹æ¢æ–‡æˆ–ç¼ºæ¼äº‹é …ï¼Œä¸¦æ˜ç¢ºèªªæ˜å…¶é•åçš„æ³•è¦æˆ–æ”¿ç­–æ¨™æº–ï¼Œä¸¦å°æ‡‰å…¶é†«ç™‚æ©Ÿæ§‹å±¤ç´šè¦æ±‚ï¼‰
"""
    response = model.generate_content(rag_prompt)
    return response.text.strip()

def extract_table_from_response(response_text, questions, facility_level="", score_cap_dict=None, repeated_subsidy_result=None, plan_result=None, legal_compliance_result=None):
    rows = []

    scores = re.findall(
        r"å¾—åˆ†\s*âŸª(\d{1,2})âŸ«\s*/1\s*[ï¼Œ,:ï¼š]\s*åŸå› [:ï¼š]?\s*(.+?)(?=\n*å¾—åˆ†\s*âŸª|\Z)",
        response_text,
        flags=re.DOTALL
    )

    if len(scores) < len(questions):
        scores += [("0", "âš ï¸ Gemini æœªå›è¦†æ­¤é¡Œ")] * (len(questions) - len(scores))
    elif len(scores) > len(questions):
        scores = scores[:len(questions)]

    for i, (score, reason) in enumerate(scores):
        cleaned_reason = reason.strip()
        rows.append({"é¡Œç›®": questions[i], "å¾—åˆ†": int(score), "åŸå› ": cleaned_reason})

    # âœ… ç¯©é™¤è™›æ“¬é¡Œç›®
    rows = [row for row in rows if not row["é¡Œç›®"].startswith("36.")]

    # âœ… ç¸½åˆ†
    total = sum(r["å¾—åˆ†"] for r in rows if isinstance(r["å¾—åˆ†"], int))
    max_score = score_cap_dict.get(facility_level.strip(), len(rows)) if score_cap_dict else len(rows)
    percent = round((total / max_score) * 100)
    rows.append({"é¡Œç›®": f"âœ… ç¸½åˆ†ï¼ˆ{facility_level}ï¼‰", "å¾—åˆ†": f"{total}/{max_score}", "åŸå› ": f"{percent}%"})

    # âœ… é‡è¤‡è£œåŠ©
    if repeated_subsidy_result:
        sub_score = 0 if "æ˜¯å¦é‡è¤‡ç”³è«‹ï¼š0" in repeated_subsidy_result else ""
        rows.append({
            "é¡Œç›®": "ğŸ“Œ æ˜¯å¦é‡è¤‡ç”³è«‹è£œåŠ©",
            "å¾—åˆ†": sub_score,
            "åŸå› ": repeated_subsidy_result
        })

    # âœ… æ©Ÿæ§‹ã€è¨ˆç•«åç¨±ã€è¨ˆç•«ä»£è™Ÿ
    if plan_result:
        org, plan_name, plan_code = parse_plan_info(plan_result)
        rows.append({"é¡Œç›®": "ğŸ¢ ç”³è«‹æ©Ÿæ§‹", "å¾—åˆ†": "", "åŸå› ": org})
        rows.append({"é¡Œç›®": "ğŸ“ è¨ˆç•«åç¨±", "å¾—åˆ†": "", "åŸå› ": plan_name})
        rows.append({"é¡Œç›®": "ğŸ†” è¨ˆç•«ç”³è«‹ç·¨è™Ÿ", "å¾—åˆ†": "", "åŸå› ": plan_code})

    # âœ… é©æ³•æ€§çµæœ
    if legal_compliance_result:
        legal_score = "ç¬¦åˆ" if "æ˜¯å¦ç¬¦åˆé©æ³•æ€§ï¼šç¬¦åˆ" in legal_compliance_result else "ä¸ç¬¦åˆ"
        rows.append({
            "é¡Œç›®": "ğŸ“Œ æ˜¯å¦ç¬¦åˆé©æ³•æ€§",
            "å¾—åˆ†": legal_score,
            "åŸå› ": legal_compliance_result
        })

    return pd.DataFrame(rows)


def render_score_table(title, response_text, questions, color, facility_level, score_cap_dict, pdf_filename, repeated_subsidy_result, plan_result,legal_compliance_result):
    df = extract_table_from_response(response_text, questions, facility_level, score_cap_dict, repeated_subsidy_result, plan_result,legal_compliance_result)
    st.markdown(f"<h4 style='color:{color}'>{title}</h4>", unsafe_allow_html=True)
    st.dataframe(df, use_container_width=True)
    csv_data = df.to_csv(index=False)
    filename = f"{pdf_filename}_è³‡è¨Šè™•.csv"
    with st.container():
        st.download_button(
            label=f"ğŸ“¥ ä¸‹è¼‰ {filename}",
            data=csv_data,
            file_name=filename,
            mime="text/csv",
            key=f"download_{pdf_filename}"  # ğŸ”‘ ä½¿ç”¨å”¯ä¸€ key é˜²æ­¢é‡æ–°è§¸ç™¼ rerun
        )
    st.markdown(f"ğŸ” Gemini å›å‚³å­—æ•¸ï¼š{len(response_text)}")
    with st.expander("ğŸ“ æŸ¥çœ‹åŸå§‹ Gemini å›æ‡‰"):
        st.write(response_text)

def parse_plan_info(plan_result_text):
    org, name, code = "â“ ç„¡æ³•åˆ¤æ–·", "â“ ç„¡æ³•åˆ¤æ–·", "â“ ç„¡æ³•åˆ¤æ–·"

    # ä½¿ç”¨ MULTILINE æ¨¡å¼ï¼Œç¢ºä¿åªæŠ“é–‹é ­çš„è¡Œ
    org_match = re.search(r"^ç”³è«‹æ©Ÿæ§‹[ï¼š:]\s*(.+)$", plan_result_text, flags=re.MULTILINE)
    name_match = re.search(r"^è¨ˆç•«åç¨±[ï¼š:]\s*(.+)$", plan_result_text, flags=re.MULTILINE)
    code_match = re.search(r"^(è¨ˆç•«ç”³è«‹ç·¨è™Ÿ|è¨ˆç•«ç·¨è™Ÿ)[ï¼š:]\s*(.+)$", plan_result_text, flags=re.MULTILINE)

    if org_match:
        org = org_match.group(1).strip()
    if name_match:
        name = name_match.group(1).strip()
    if code_match:
        code = code_match.group(2).strip()

    return org, name, code

def main():
    st.set_page_config("ğŸ“„ å¤š PDF è‡ªå‹•åˆ†æå™¨", layout="wide")
    st.title("ğŸ“„ å¤šä»½ PDF è‡ªå‹•åˆ†æèˆ‡è³‡è¨Šè™•è©•åˆ†")
    uploaded_pdfs = st.file_uploader("ğŸ“¥ ä¸Šå‚³ PDF æ–‡ä»¶ï¼ˆå¯è¤‡é¸ï¼‰", type=["pdf"], accept_multiple_files=True)
    use_split = st.checkbox("âœ… å•Ÿç”¨é€é¡Œåˆ†æï¼ˆå»ºè­°ç”¨æ–¼é•·æ–‡ä»¶ï¼‰")
    if uploaded_pdfs and st.button("ğŸš€ é–‹å§‹åˆ†æ"):
        for uploaded_pdf in uploaded_pdfs:
            pdf_bytes = uploaded_pdf.read()
            pdf_filename = uploaded_pdf.name.rsplit(".", 1)[0]
            with st.spinner(f"â³ åˆ†æä¸­ï¼š{uploaded_pdf.name}"):
                # âœ… è‡ªå‹•åˆ¤æ–·æ˜¯å¦ç‚ºæƒæ PDF
                full_text = extract_text_by_line(pdf_bytes)
                facility_prompt = FACILITY_PROMPT.replace("{text}", full_text)
                facility_type = get_gemini_response(facility_prompt).strip()
                plan_prompt = PLAN_PROMPT.replace("{text}", full_text)
                plan_info_result = get_gemini_response(plan_prompt)
                legal_compliance_result = query_rag_for_legal_compliance(full_text)
                
                st.info(f"ğŸ¥ æª”æ¡ˆï¼š{uploaded_pdf.name} â†’ æ©Ÿæ§‹åˆ†é¡ï¼š{facility_type}")
                if use_split:
                    prompts = build_individual_prompts(INFO_QUESTIONS, full_text, "è³‡è¨Šè™•å¯©æŸ¥å§”å“¡")
                    responses = [get_gemini_response(p) for p in prompts]
                    info_result = "\n".join(responses)
                    subsidy_prompt = REPEATED_SUBSIDY_PROMPT.replace("{text}", full_text)
                    repeated_subsidy_result = get_gemini_response(subsidy_prompt)
                else:
                    prompt = build_individual_prompts(INFO_QUESTIONS, full_text)[0].replace(
                        "é¡Œç›®ï¼š" + INFO_QUESTIONS[0] + "\n\n",
                        "è«‹ä¾ç…§ä»¥ä¸‹é¡Œç›®é †åºé€ä¸€å›ç­”æ¯ä¸€é¡Œï¼š\n\n" + "\n".join(INFO_QUESTIONS)
                    )
                    info_result = get_gemini_response(prompt)
                    subsidy_prompt = REPEATED_SUBSIDY_PROMPT.replace("{text}", full_text)
                    repeated_subsidy_result = get_gemini_response(subsidy_prompt)
            st.success(f"âœ… {uploaded_pdf.name} åˆ†æå®Œæˆ")
            render_score_table(
    "ğŸ’» è³‡è¨Šè™•è©•åˆ†è¡¨",
    info_result,
    INFO_QUESTIONS,
    "#1f77b4",
    facility_type,
    FACILITY_SCORE_CAP_INFO,
    pdf_filename,
    repeated_subsidy_result,
    plan_info_result,legal_compliance_result
)

if __name__ == "__main__":
    main()

